import feedparser
from datetime import datetime, timedelta
from typing import List, Dict
import time

class NewsCollector:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # è»Šé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ
        self.car_feeds = {
            'Car Watch': 'https://car.watch.impress.co.jp/data/rss/1.0/cw/feed.rdf',
            'Response': 'https://response.jp/rss/index.rdf',
            'Autoblog Japan': 'https://jp.autoblog.com/rss.xml',
        }

        # ITé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ
        self.it_feeds = {
            'ITmedia News': 'https://www.itmedia.co.jp/news/rss/rss2.xml',
            'ITmedia AI+': 'https://rss.itmedia.co.jp/rss/2.0/aiplus.xml',
            '@IT': 'https://rss.itmedia.co.jp/rss/2.0/ait.xml',
            'Publickey': 'https://www.publickey1.jp/atom.xml',
            'GIZMODO Japan': 'https://www.gizmodo.jp/index.xml',
            'TechCrunch Japan': 'https://techcrunch.com/feed/',
            'Engadgetæ—¥æœ¬ç‰ˆ': 'https://japanese.engadget.com/rss.xml',
            'CNET Japan': 'https://japan.cnet.com/rss/index.rdf',
            'Zenn': 'https://zenn.dev/feed',
            'Qiita (JavaScript)': 'https://qiita.com/tags/JavaScript/feed.atom',
            'Qiita (Python)': 'https://qiita.com/tags/Python/feed.atom',
            'Qiita (React)': 'https://qiita.com/tags/React/feed.atom',
        }

        # ã™ã¹ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’çµ±åˆï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
        self.rss_feeds = {**self.car_feeds, **self.it_feeds}

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«è‡ªå‹•è»Šãƒ¡ãƒ¼ã‚«ãƒ¼ã®å…¬å¼RSS
        self.manufacturer_feeds = {
            'Toyota Global': 'https://global.toyota/en/newsroom/rss/',
            'Honda': 'https://global.honda/en/newsroom/rss/news.xml',
            'Tesla': 'https://www.tesla.com/blog/rss',
        }

    def fetch_recent_news(self, hours_back: int = 24) -> List[Dict]:
        """
        éå»Næ™‚é–“ä»¥å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—

        Args:
            hours_back: ä½•æ™‚é–“å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹

        Returns:
            è¨˜äº‹ã®ãƒªã‚¹ãƒˆï¼ˆã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å«ã‚€ï¼‰
        """
        articles = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        # è»Šé–¢é€£è¨˜äº‹ã‚’å–å¾—
        for source_name, feed_url in self.car_feeds.items():
            articles.extend(self._fetch_from_feed(source_name, feed_url, cutoff_time, 'car'))

        # ITé–¢é€£è¨˜äº‹ã‚’å–å¾—
        for source_name, feed_url in self.it_feeds.items():
            articles.extend(self._fetch_from_feed(source_name, feed_url, cutoff_time, 'it'))

        # ãƒ¡ãƒ¼ã‚«ãƒ¼å…¬å¼è¨˜äº‹ã‚’å–å¾—ï¼ˆè»Šã‚«ãƒ†ã‚´ãƒªï¼‰
        for source_name, feed_url in self.manufacturer_feeds.items():
            articles.extend(self._fetch_from_feed(source_name, feed_url, cutoff_time, 'car'))

        # å…¬é–‹æ—¥æ™‚é †ã«ã‚½ãƒ¼ãƒˆ
        articles.sort(key=lambda x: x['published'], reverse=True)

        print(f"\nâœ… åˆè¨ˆ {len(articles)} ä»¶ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
        print(f"   - è»Š: {len([a for a in articles if a['category'] == 'car'])} ä»¶")
        print(f"   - IT: {len([a for a in articles if a['category'] == 'it'])} ä»¶\n")

        return articles

    def _fetch_from_feed(self, source_name: str, feed_url: str, cutoff_time: datetime, category: str) -> List[Dict]:
        """
        å˜ä¸€ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—

        Args:
            source_name: æƒ…å ±æºå
            feed_url: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®URL
            cutoff_time: ã“ã®æ—¥æ™‚ã‚ˆã‚Šæ–°ã—ã„è¨˜äº‹ã®ã¿å–å¾—
            category: 'car' ã¾ãŸã¯ 'it'

        Returns:
            è¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """
        articles = []

        try:
            print(f"ğŸ“¡ {source_name} ({category}) ã‹ã‚‰å–å¾—ä¸­...")
            feed = feedparser.parse(feed_url)

            for entry in feed.entries:
                try:
                    # å…¬é–‹æ—¥æ™‚ã®å–å¾—ï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰å½¢å¼ã«ã‚ˆã‚Šç•°ãªã‚‹ï¼‰
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        pub_date = datetime(*entry.updated_parsed[:6])
                    else:
                        pub_date = datetime.now()

                    # æŒ‡å®šæ™‚é–“å†…ã®è¨˜äº‹ã®ã¿è¿½åŠ 
                    if pub_date >= cutoff_time:
                        article = {
                            'title': entry.title,
                            'url': entry.link,
                            'summary': entry.get('summary', entry.get('description', ''))[:500],
                            'published': pub_date.isoformat(),
                            'source': source_name,
                            'category': category,
                        }
                        articles.append(article)

                except Exception as e:
                    print(f"  âš ï¸  è¨˜äº‹ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    continue

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆå°‘ã—å¾…æ©Ÿï¼‰
            time.sleep(0.5)

        except Exception as e:
            print(f"  âŒ {source_name} ã®å–å¾—ã«å¤±æ•—: {str(e)[:50]}")

        return articles

    def get_manufacturer_news_only(self, hours_back: int = 48) -> List[Dict]:
        """
        ãƒ¡ãƒ¼ã‚«ãƒ¼å…¬å¼ã®æ–°å‹è»Šæƒ…å ±ã®ã¿ã‚’å–å¾—ï¼ˆã‚ˆã‚Šé•·ã„æœŸé–“ï¼‰

        Args:
            hours_back: ä½•æ™‚é–“å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ48æ™‚é–“ï¼‰

        Returns:
            è¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """
        articles = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        for source_name, feed_url in self.manufacturer_feeds.items():
            try:
                print(f"ğŸ­ {source_name} å…¬å¼æƒ…å ±ã‚’å–å¾—ä¸­...")
                feed = feedparser.parse(feed_url)

                for entry in feed.entries:
                    try:
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_date = datetime(*entry.published_parsed[:6])
                        else:
                            pub_date = datetime.now()

                        if pub_date >= cutoff_time:
                            article = {
                                'title': entry.title,
                                'url': entry.link,
                                'summary': entry.get('summary', entry.get('description', ''))[:500],
                                'published': pub_date.isoformat(),
                                'source': source_name,
                            }
                            articles.append(article)

                    except Exception as e:
                        continue

                time.sleep(0.5)

            except Exception as e:
                print(f"  âŒ {source_name} ã®å–å¾—ã«å¤±æ•—: {str(e)[:50]}")
                continue

        return articles
