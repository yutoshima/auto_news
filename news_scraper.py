#!/usr/bin/env python3
"""
News Scraper for Automobile Manufacturers

robots.txtè¨±å¯æ¸ˆã¿ã®ãƒ¡ãƒ¼ã‚«ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ ã‹ã‚‰è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional
from datetime import datetime
import time
import re


class NewsScraper:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        })

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ãƒ¡ãƒ¼ã‚«ãƒ¼ã®è¨­å®š
        self.scraping_configs = {
            'Mercedes-Benz': {
                'url': 'https://media.mercedes-benz.com/news',
                'article_selector': 'article.news-item, div.news-item, div.article-item',
                'title_selector': 'h2, h3, .title, .headline',
                'link_selector': 'a',
                'date_selector': 'time, .date, .published',
                'summary_selector': 'p, .summary, .description',
                'max_articles': 20
            },
            'Audi': {
                'url': 'https://www.audi-mediacenter.com/en/press-releases',
                'article_selector': 'li.detailed-page-list-item',
                'title_selector': 'a',
                'link_selector': 'a',
                'date_selector': 'time, .date, span.date',
                'summary_selector': 'p, .teaser, .summary',
                'max_articles': 20
            },
            'Volkswagen': {
                'url': 'https://www.volkswagen-newsroom.com/en/press-releases',
                'article_selector': 'h3.page-preview--title',
                'title_selector': 'a',
                'link_selector': 'a',
                'date_selector': 'time, .date',
                'summary_selector': 'p, .summary',
                'max_articles': 20
            },
            'Jaguar': {
                'url': 'https://media.jaguar.com/news',
                'article_selector': 'article, div.news-item',
                'title_selector': 'h2, h3, .title',
                'link_selector': 'a',
                'date_selector': 'time, .date',
                'summary_selector': 'p, .summary',
                'max_articles': 20
            },
            'Land Rover': {
                'url': 'https://media.landrover.com/news',
                'article_selector': 'article, div.news-item',
                'title_selector': 'h2, h3, .title',
                'link_selector': 'a',
                'date_selector': 'time, .date',
                'summary_selector': 'p, .summary',
                'max_articles': 20
            },
        }

    def analyze_html_structure(self, manufacturer: str) -> Dict:
        """
        ãƒ¡ãƒ¼ã‚«ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®HTMLæ§‹é€ ã‚’åˆ†æ

        Args:
            manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼å

        Returns:
            åˆ†æçµæœã®è¾æ›¸
        """
        if manufacturer not in self.scraping_configs:
            return {'error': f'{manufacturer} is not in scraping configs'}

        config = self.scraping_configs[manufacturer]
        url = config['url']

        print(f"\n{'='*60}")
        print(f"ğŸ” {manufacturer} ã®HTMLæ§‹é€ ã‚’åˆ†æä¸­...")
        print(f"   URL: {url}")
        print(f"{'='*60}\n")

        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ä¸€èˆ¬çš„ãªè¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ç´¢
            potential_selectors = [
                'article',
                'div.news-item',
                'div.press-release',
                'div.article-item',
                'div[class*="news"]',
                'div[class*="article"]',
                'li.news-item',
                'div[class*="press"]',
            ]

            found_elements = {}
            for selector in potential_selectors:
                elements = soup.select(selector)
                if elements:
                    found_elements[selector] = len(elements)
                    print(f"âœ… ç™ºè¦‹: '{selector}' - {len(elements)}å€‹")

            # ã‚¿ã‚¤ãƒˆãƒ«è¦ç´ ã®åˆ†æ
            print(f"\nğŸ“° ã‚¿ã‚¤ãƒˆãƒ«è¦ç´ ã‚’æ¢ç´¢ä¸­...")
            title_selectors = ['h1', 'h2', 'h3', '.title', '.headline', 'a.title']
            for selector in title_selectors:
                titles = soup.select(selector)
                if titles:
                    print(f"   '{selector}' - {len(titles)}å€‹")
                    if len(titles) > 0 and len(titles) < 50:  # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦å¦¥å½“ãªæ•°
                        print(f"      ä¾‹: {titles[0].get_text(strip=True)[:60]}...")

            # æ—¥ä»˜è¦ç´ ã®åˆ†æ
            print(f"\nğŸ“… æ—¥ä»˜è¦ç´ ã‚’æ¢ç´¢ä¸­...")
            date_selectors = ['time', '.date', '.published', 'span.date', '[datetime]']
            for selector in date_selectors:
                dates = soup.select(selector)
                if dates:
                    print(f"   '{selector}' - {len(dates)}å€‹")
                    if dates:
                        print(f"      ä¾‹: {dates[0].get_text(strip=True)[:40]}")

            # ãƒªãƒ³ã‚¯è¦ç´ ã®åˆ†æ
            print(f"\nğŸ”— ãƒªãƒ³ã‚¯æ§‹é€ ã‚’åˆ†æä¸­...")
            all_links = soup.find_all('a', href=True)
            news_links = [a for a in all_links if any(
                keyword in a.get('href', '').lower()
                for keyword in ['news', 'press', 'release', 'article']
            )]
            print(f"   å…¨ãƒªãƒ³ã‚¯: {len(all_links)}å€‹")
            print(f"   ãƒ‹ãƒ¥ãƒ¼ã‚¹é–¢é€£ãƒªãƒ³ã‚¯: {len(news_links)}å€‹")

            return {
                'manufacturer': manufacturer,
                'url': url,
                'found_selectors': found_elements,
                'total_links': len(all_links),
                'news_links': len(news_links),
                'status': 'success'
            }

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'manufacturer': manufacturer,
                'url': url,
                'error': str(e),
                'status': 'failed'
            }

    def scrape_news(self, manufacturer: str, hours_back: int = 48) -> List[Dict]:
        """
        ç‰¹å®šãƒ¡ãƒ¼ã‚«ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

        Args:
            manufacturer: ãƒ¡ãƒ¼ã‚«ãƒ¼å
            hours_back: ä½•æ™‚é–“å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹

        Returns:
            è¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """
        if manufacturer not in self.scraping_configs:
            print(f"âš ï¸  {manufacturer} ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®šãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        config = self.scraping_configs[manufacturer]
        url = config['url']

        print(f"ğŸŒ {manufacturer} ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­... ({url})")

        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¤œç´¢
            articles = soup.select(config['article_selector'])

            if not articles:
                print(f"   âš ï¸  è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚»ãƒ¬ã‚¯ã‚¿: {config['article_selector']}ï¼‰")
                return []

            print(f"   ğŸ“° {len(articles)} ä»¶ã®è¨˜äº‹å€™è£œã‚’ç™ºè¦‹")

            scraped_articles = []

            for i, article in enumerate(articles[:config['max_articles']], 1):
                try:
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title_elem = article.select_one(config['title_selector'])
                    if not title_elem:
                        continue
                    title = title_elem.get_text(strip=True)

                    # ãƒªãƒ³ã‚¯ã‚’å–å¾—
                    link_elem = article.select_one(config['link_selector'])
                    if not link_elem or not link_elem.get('href'):
                        continue
                    link = urljoin(url, link_elem.get('href'))

                    # è¦ç´„ã‚’å–å¾—
                    summary_elem = article.select_one(config['summary_selector'])
                    summary = summary_elem.get_text(strip=True)[:500] if summary_elem else ''

                    # æ—¥ä»˜ã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    date_elem = article.select_one(config['date_selector'])
                    pub_date = datetime.now().isoformat()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç¾åœ¨æ™‚åˆ»
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                        # æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã¯å¾Œã§å®Ÿè£…å¯èƒ½
                        pub_date = datetime.now().isoformat()

                    article_data = {
                        'title': title,
                        'url': link,
                        'summary': summary,
                        'published': pub_date,
                        'source': manufacturer,
                        'category': 'car',
                        'scraping': True  # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—ã—ãŸã“ã¨ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°
                    }

                    scraped_articles.append(article_data)

                except Exception as e:
                    print(f"      âš ï¸  è¨˜äº‹{i}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    continue

            print(f"   âœ… {len(scraped_articles)} ä»¶ã®è¨˜äº‹ã‚’å–å¾—å®Œäº†")

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆ2-3ç§’å¾…æ©Ÿï¼‰
            time.sleep(2.5)

            return scraped_articles

        except Exception as e:
            print(f"   âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {str(e)}")
            return []

    def scrape_all(self, hours_back: int = 48) -> Dict[str, List[Dict]]:
        """
        å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

        Args:
            hours_back: ä½•æ™‚é–“å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹

        Returns:
            ãƒ¡ãƒ¼ã‚«ãƒ¼åã‚’ã‚­ãƒ¼ã¨ã—ãŸè¨˜äº‹ãƒªã‚¹ãƒˆã®è¾æ›¸
        """
        print(f"\nğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ï¼ˆå¯¾è±¡: {len(self.scraping_configs)} ç¤¾ï¼‰\n")

        results = {}

        for manufacturer in self.scraping_configs.keys():
            articles = self.scrape_news(manufacturer, hours_back)
            results[manufacturer] = articles

        total_articles = sum(len(articles) for articles in results.values())
        print(f"\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: åˆè¨ˆ {total_articles} ä»¶ã®è¨˜äº‹ã‚’å–å¾—\n")

        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†: HTMLæ§‹é€ åˆ†æ"""
    scraper = NewsScraper()

    print("ğŸ” è‡ªå‹•è»Šãƒ¡ãƒ¼ã‚«ãƒ¼ HTMLæ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«")
    print("="*60)
    print()

    # å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã®HTMLæ§‹é€ ã‚’åˆ†æ
    for manufacturer in scraper.scraping_configs.keys():
        result = scraper.analyze_html_structure(manufacturer)
        time.sleep(3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™

    print("\n" + "="*60)
    print("âœ¨ åˆ†æå®Œäº†")
    print("="*60)


if __name__ == "__main__":
    main()
