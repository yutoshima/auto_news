#!/usr/bin/env python3
"""
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯å¦ç¢ºèªãƒ„ãƒ¼ãƒ«

å„ãƒ¡ãƒ¼ã‚«ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ ãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½ã‹ç¢ºèªã—ã¾ã™ï¼š
- robots.txtã®ç¢ºèª
- ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
- HTMLæ§‹é€ ã®ç¢ºèª
"""

import requests
import urllib.robotparser
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import time

class ScrapingChecker:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯å¦ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'AutoNewsBot/1.0 (Educational Purpose; Checking Scraping Feasibility)',
            'Accept': 'text/html,application/xhtml+xml',
        })

        # ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ãƒ¡ãƒ¼ã‚«ãƒ¼
        self.manufacturers = {
            'Nissan': 'https://global.nissannews.com/en/releases',
            'Mercedes-Benz': 'https://media.mercedes-benz.com/news',
            'BMW': 'https://www.press.bmwgroup.com/global/article/detail',
            'Audi': 'https://www.audi-mediacenter.com/en/press-releases',
            'Volkswagen': 'https://www.volkswagen-newsroom.com/en/press-releases',
            'Chevrolet': 'https://media.chevrolet.com/media/us/en/chevrolet/news.html',
            'Cadillac': 'https://media.cadillac.com/media/us/en/cadillac/news.html',
            'Jaguar': 'https://media.jaguarlandrover.com/news',
            'Land Rover': 'https://media.jaguarlandrover.com/news',
            'Volvo': 'https://www.media.volvocars.com/global/en-gb/media/pressreleases',
        }

    def check_robots_txt(self, base_url: str, path: str) -> dict:
        """
        robots.txtã‚’ç¢ºèª

        Returns:
            {
                'allowed': bool,
                'robots_url': str,
                'content': str or None
            }
        """
        result = {
            'allowed': False,
            'robots_url': urljoin(base_url, '/robots.txt'),
            'content': None,
            'error': None
        }

        try:
            rp = urllib.robotparser.RobotFileParser()
            rp.set_url(result['robots_url'])
            rp.read()

            user_agent = self.session.headers['User-Agent']
            result['allowed'] = rp.can_fetch(user_agent, urljoin(base_url, path))

            # robots.txtã®å†…å®¹ã‚’å–å¾—
            try:
                robots_response = requests.get(result['robots_url'], timeout=5)
                if robots_response.status_code == 200:
                    result['content'] = robots_response.text[:500]  # æœ€åˆã®500æ–‡å­—
            except:
                pass

        except Exception as e:
            result['error'] = str(e)
            result['allowed'] = False  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ã®ãŸã‚False

        return result

    def check_page_access(self, url: str) -> dict:
        """
        ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ç¢ºèª

        Returns:
            {
                'accessible': bool,
                'status_code': int or None,
                'error': str or None
            }
        """
        result = {
            'accessible': False,
            'status_code': None,
            'error': None,
            'has_articles': False
        }

        try:
            response = self.session.get(url, timeout=10)
            result['status_code'] = response.status_code
            result['accessible'] = response.status_code == 200

            if result['accessible']:
                # ç°¡æ˜“çš„ãªè¨˜äº‹è¦ç´ ãƒã‚§ãƒƒã‚¯
                soup = BeautifulSoup(response.content, 'html.parser')

                # ã‚ˆãã‚ã‚‹è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                article_patterns = [
                    'article', 'div.news', 'div.press-release',
                    'div.release', 'li.news-item', 'div.story'
                ]

                for pattern in article_patterns:
                    elements = soup.select(pattern)
                    if len(elements) > 0:
                        result['has_articles'] = True
                        result['article_count'] = len(elements)
                        result['article_pattern'] = pattern
                        break

        except requests.RequestException as e:
            result['error'] = str(e)
        except Exception as e:
            result['error'] = f"Unexpected error: {str(e)}"

        return result

    def check_manufacturer(self, name: str, url: str) -> dict:
        """
        ç‰¹å®šã®ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯

        Returns:
            ç·åˆçš„ãªãƒã‚§ãƒƒã‚¯çµæœ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ” {name} ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
        print(f"   URL: {url}")

        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"

        # robots.txtãƒã‚§ãƒƒã‚¯
        print(f"\nğŸ“„ robots.txt ã‚’ç¢ºèªä¸­...")
        robots_result = self.check_robots_txt(base_url, parsed_url.path)

        if robots_result['allowed']:
            print(f"   âœ… ã‚¯ãƒ­ãƒ¼ãƒ«è¨±å¯")
        else:
            print(f"   âŒ ã‚¯ãƒ­ãƒ¼ãƒ«ç¦æ­¢")
            if robots_result['error']:
                print(f"   âš ï¸  ã‚¨ãƒ©ãƒ¼: {robots_result['error']}")

        if robots_result['content']:
            print(f"   ğŸ“ robots.txt (æŠœç²‹):")
            for line in robots_result['content'].split('\n')[:5]:
                if line.strip():
                    print(f"      {line}")

        # ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ãƒã‚§ãƒƒã‚¯
        print(f"\nğŸŒ ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ã‚’ç¢ºèªä¸­...")
        access_result = self.check_page_access(url)

        if access_result['accessible']:
            print(f"   âœ… ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ (Status: {access_result['status_code']})")
            if access_result['has_articles']:
                print(f"   ğŸ“° è¨˜äº‹è¦ç´ ã‚’ç™ºè¦‹: {access_result.get('article_count', 0)} ä»¶")
                print(f"   ğŸ¯ ãƒ‘ã‚¿ãƒ¼ãƒ³: {access_result.get('article_pattern', 'N/A')}")
            else:
                print(f"   âš ï¸  è¨˜äº‹è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        else:
            print(f"   âŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯")
            if access_result['error']:
                print(f"   âš ï¸  ã‚¨ãƒ©ãƒ¼: {access_result['error']}")

        # ç·åˆåˆ¤å®š
        feasible = robots_result['allowed'] and access_result['accessible']

        print(f"\n{'â”€'*60}")
        if feasible:
            print(f"âœ… {name}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½")
            if access_result['has_articles']:
                print(f"   æ¨å¥¨åº¦: â­â­â­ (è¨˜äº‹è¦ç´ ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ)")
            else:
                print(f"   æ¨å¥¨åº¦: â­â­â˜† (HTMLæ§‹é€ ã®è©³ç´°èª¿æŸ»ãŒå¿…è¦)")
        else:
            print(f"âŒ {name}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°éæ¨å¥¨")
            if not robots_result['allowed']:
                print(f"   ç†ç”±: robots.txtã§ç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™")
            if not access_result['accessible']:
                print(f"   ç†ç”±: ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“")

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(2)

        return {
            'name': name,
            'url': url,
            'feasible': feasible,
            'robots': robots_result,
            'access': access_result
        }

    def check_all(self) -> list:
        """å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯"""
        print("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯å¦ãƒã‚§ãƒƒã‚¯ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"å¯¾è±¡ãƒ¡ãƒ¼ã‚«ãƒ¼: {len(self.manufacturers)} ç¤¾\n")

        results = []

        for name, url in self.manufacturers.items():
            result = self.check_manufacturer(name, url)
            results.append(result)

        return results

    def print_summary(self, results: list):
        """ãƒã‚§ãƒƒã‚¯çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š ãƒã‚§ãƒƒã‚¯çµæœã‚µãƒãƒªãƒ¼")
        print("="*60)

        feasible = [r for r in results if r['feasible']]
        not_feasible = [r for r in results if not r['feasible']]

        print(f"\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½: {len(feasible)} ç¤¾")
        for r in feasible:
            has_articles = r['access'].get('has_articles', False)
            star = "â­â­â­" if has_articles else "â­â­â˜†"
            print(f"   {star} {r['name']}")

        print(f"\nâŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°éæ¨å¥¨: {len(not_feasible)} ç¤¾")
        for r in not_feasible:
            print(f"   â€¢ {r['name']}")

        print(f"\nğŸ“ˆ æˆåŠŸç‡: {len(feasible)}/{len(results)} ({len(feasible)/len(results)*100:.1f}%)")

        print("\n" + "="*60)
        print("ğŸ¯ æ¨å¥¨äº‹é …")
        print("="*60)

        if feasible:
            print("\nâœ… ä»¥ä¸‹ã®ãƒ¡ãƒ¼ã‚«ãƒ¼ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½ã§ã™ï¼š")
            for r in feasible:
                print(f"\n   ã€{r['name']}ã€‘")
                print(f"   URL: {r['url']}")
                if r['access'].get('has_articles'):
                    print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³: {r['access'].get('article_pattern')}")
                    print(f"   è¨˜äº‹æ•°: {r['access'].get('article_count')} ä»¶")
                print(f"   æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: HTMLæ§‹é€ ã®è©³ç´°åˆ†æ")

        if not_feasible:
            print("\nâš ï¸  ä»¥ä¸‹ã®ãƒ¡ãƒ¼ã‚«ãƒ¼ã¯ä»£æ›¿æ‰‹æ®µã‚’æ¤œè¨ã—ã¦ãã ã•ã„ï¼š")
            for r in not_feasible:
                print(f"\n   ã€{r['name']}ã€‘")
                if not r['robots']['allowed']:
                    print(f"   å•é¡Œ: robots.txtã§ç¦æ­¢")
                    print(f"   ä»£æ›¿æ¡ˆ: å…¬å¼RSS / å…¬å¼API ã‚’æ¢ã™")
                if not r['access']['accessible']:
                    print(f"   å•é¡Œ: ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯")
                    print(f"   ä»£æ›¿æ¡ˆ: URL ã‚’ç¢ºèªã€ã¾ãŸã¯åˆ¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ¢ã™")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    checker = ScrapingChecker()

    print("ğŸ¤– è‡ªå‹•è»Šãƒ¡ãƒ¼ã‚«ãƒ¼ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯å¦ãƒã‚§ãƒƒã‚¯ãƒ„ãƒ¼ãƒ«")
    print("="*60)
    print()

    # å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
    results = checker.check_all()

    # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    checker.print_summary(results)

    print("\nâœ¨ ãƒã‚§ãƒƒã‚¯å®Œäº†")


if __name__ == "__main__":
    main()
