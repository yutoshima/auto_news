#!/usr/bin/env python3
"""
RSS URL Discovery Tool

å„ãƒ¡ãƒ¼ã‚«ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ ã‹ã‚‰è‡ªå‹•çš„ã«RSS URLã‚’æ¢ç´¢ã—ã¾ã™ã€‚
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time


class RSSDiscovery:
    """RSS URLã‚’è‡ªå‹•ç™ºè¦‹ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        })

        # èª¿æŸ»å¯¾è±¡ã®ãƒ¡ãƒ¼ã‚«ãƒ¼ï¼ˆRSSå–å¾—å¤±æ•—ã®11ç¤¾ï¼‰
        self.manufacturers = {
            'Toyota': 'https://global.toyota/en/newsroom/',
            'Lexus': 'https://pressroom.lexus.com/',
            'Honda': 'https://global.honda/en/newsroom/',
            'Mazda': 'https://www.mazda.com/en/',
            'Porsche': 'https://newsroom.porsche.com/en/',
            'General Motors': 'https://news.gm.com/',
            'Ford': 'https://media.ford.com/',
            'Tesla': 'https://ir.tesla.com/',
            'Hyundai': 'https://www.hyundainews.com/',
            'Lamborghini': 'https://media.lamborghini.com/',
            'Rolls-Royce': 'https://www.press.rolls-roycemotorcars.com/',
        }

    def discover_rss_url(self, name: str, base_url: str) -> dict:
        """
        ç‰¹å®šã®ãƒ¡ãƒ¼ã‚«ãƒ¼ã®RSS URLã‚’ç™ºè¦‹

        Returns:
            {
                'found': bool,
                'rss_urls': list,
                'methods': list,  # ã©ã®æ–¹æ³•ã§ç™ºè¦‹ã—ãŸã‹
                'page_url': str
            }
        """
        result = {
            'name': name,
            'base_url': base_url,
            'found': False,
            'rss_urls': [],
            'methods': [],
            'error': None
        }

        print(f"\n{'='*60}")
        print(f"ğŸ” {name} ã®RSS URLã‚’æ¢ç´¢ä¸­...")
        print(f"   URL: {base_url}")
        print(f"{'='*60}")

        try:
            # Method 1: HTML <link> ã‚¿ã‚°ã‹ã‚‰æ¤œç´¢
            print(f"\nğŸ“„ Method 1: HTML <link> ã‚¿ã‚°ã‚’ç¢ºèªä¸­...")
            link_rss = self._find_rss_in_link_tags(base_url)
            if link_rss:
                result['rss_urls'].extend(link_rss)
                result['methods'].append('link_tag')
                result['found'] = True
                for url in link_rss:
                    print(f"   âœ… ç™ºè¦‹: {url}")

            # Method 2: ã‚ˆãã‚ã‚‹RSS URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
            print(f"\nğŸ“‹ Method 2: ä¸€èˆ¬çš„ãªRSSãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œä¸­...")
            common_rss = self._try_common_rss_patterns(base_url)
            if common_rss:
                for url in common_rss:
                    if url not in result['rss_urls']:
                        result['rss_urls'].append(url)
                        result['methods'].append('common_pattern')
                        result['found'] = True
                        print(f"   âœ… ç™ºè¦‹: {url}")

            # Method 3: ãƒšãƒ¼ã‚¸å†…ã®RSSãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            print(f"\nğŸ”— Method 3: ãƒšãƒ¼ã‚¸å†…ã®RSSãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ä¸­...")
            page_rss = self._find_rss_links_in_page(base_url)
            if page_rss:
                for url in page_rss:
                    if url not in result['rss_urls']:
                        result['rss_urls'].append(url)
                        result['methods'].append('page_link')
                        result['found'] = True
                        print(f"   âœ… ç™ºè¦‹: {url}")

            # çµæœã‚µãƒãƒªãƒ¼
            print(f"\n{'-'*60}")
            if result['found']:
                print(f"âœ… {name}: {len(result['rss_urls'])} å€‹ã®RSS URLã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
                for i, url in enumerate(result['rss_urls'], 1):
                    print(f"   {i}. {url}")
            else:
                print(f"âŒ {name}: RSS URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        except Exception as e:
            result['error'] = str(e)
            print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼: {str(e)}")

        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        return result

    def _find_rss_in_link_tags(self, url: str) -> list:
        """HTML <link> ã‚¿ã‚°ã‹ã‚‰RSS URLã‚’æ¤œç´¢"""
        rss_urls = []

        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # <link rel="alternate" type="application/rss+xml"> ã‚’æ¢ã™
            rss_links = soup.find_all('link', {
                'rel': 'alternate',
                'type': lambda x: x and 'rss' in x.lower() or x and 'atom' in x.lower()
            })

            for link in rss_links:
                href = link.get('href')
                if href:
                    full_url = urljoin(url, href)
                    rss_urls.append(full_url)

        except Exception as e:
            pass

        return rss_urls

    def _try_common_rss_patterns(self, base_url: str) -> list:
        """ã‚ˆãã‚ã‚‹RSS URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™"""
        rss_urls = []

        parsed = urlparse(base_url)
        base = f"{parsed.scheme}://{parsed.netloc}"

        # ä¸€èˆ¬çš„ãªRSSãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            '/rss',
            '/feed',
            '/rss.xml',
            '/feed.xml',
            '/news/rss',
            '/news/feed',
            '/press-releases/rss',
            '/press-releases/feed',
            '/en/rss',
            '/en/feed',
            '/global/rss',
            '/global/feed',
            '/newsroom/rss',
            '/newsroom/feed',
        ]

        for pattern in patterns:
            test_url = base + pattern

            try:
                response = self.session.head(test_url, timeout=5, allow_redirects=True)

                # 200 OKã¾ãŸã¯RSS/XMLã®Content-Type
                if response.status_code == 200:
                    content_type = response.headers.get('Content-Type', '').lower()
                    if 'xml' in content_type or 'rss' in content_type or 'atom' in content_type:
                        rss_urls.append(test_url)
                        print(f"      è©¦è¡Œ: {pattern} â†’ âœ…")
                        continue

                # HEADã§å¤±æ•—ã—ãŸå ´åˆã¯GETã‚‚è©¦ã™
                response = self.session.get(test_url, timeout=5)
                if response.status_code == 200:
                    # XMLã£ã½ã„å†…å®¹ã‹ç¢ºèª
                    if b'<?xml' in response.content[:100] or b'<rss' in response.content[:500]:
                        rss_urls.append(test_url)
                        print(f"      è©¦è¡Œ: {pattern} â†’ âœ…")

            except:
                pass

        return rss_urls

    def _find_rss_links_in_page(self, url: str) -> list:
        """ãƒšãƒ¼ã‚¸å†…ã®RSSãƒªãƒ³ã‚¯ã‚’æ¤œç´¢"""
        rss_urls = []

        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # "RSS" ã¾ãŸã¯ "Feed" ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            all_links = soup.find_all('a', href=True)

            for link in all_links:
                href = link.get('href', '')
                text = link.get_text().lower()

                # RSSã£ã½ã„ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                if ('rss' in href.lower() or 'feed' in href.lower() or
                    'rss' in text or 'feed' in text):

                    full_url = urljoin(url, href)

                    # .xml, .rss ã§çµ‚ã‚ã‚‹ã€ã¾ãŸã¯rss/feedã‚’å«ã‚€URL
                    if (full_url.endswith(('.xml', '.rss')) or
                        '/rss' in full_url or '/feed' in full_url):

                        if full_url not in rss_urls:
                            rss_urls.append(full_url)

        except Exception as e:
            pass

        return rss_urls

    def discover_all(self) -> dict:
        """å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã®RSS URLã‚’æ¢ç´¢"""
        print("ğŸš€ RSS URLè‡ªå‹•æ¢ç´¢ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"å¯¾è±¡ãƒ¡ãƒ¼ã‚«ãƒ¼: {len(self.manufacturers)} ç¤¾\n")

        results = {}

        for name, url in self.manufacturers.items():
            result = self.discover_rss_url(name, url)
            results[name] = result

        return results

    def print_summary(self, results: dict):
        """æ¢ç´¢çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š RSS URLæ¢ç´¢çµæœã‚µãƒãƒªãƒ¼")
        print("="*60)

        found_count = sum(1 for r in results.values() if r['found'])
        total_count = len(results)

        print(f"\nâœ… RSS URLç™ºè¦‹: {found_count}/{total_count} ç¤¾")

        # ç™ºè¦‹ã—ãŸãƒ¡ãƒ¼ã‚«ãƒ¼
        if found_count > 0:
            print(f"\nã€ç™ºè¦‹æ¸ˆã¿ã€‘")
            for name, result in results.items():
                if result['found']:
                    print(f"\nğŸ‰ {name}")
                    for i, url in enumerate(result['rss_urls'], 1):
                        print(f"   {i}. {url}")
                    print(f"   ç™ºè¦‹æ–¹æ³•: {', '.join(set(result['methods']))}")

        # æœªç™ºè¦‹ã®ãƒ¡ãƒ¼ã‚«ãƒ¼
        not_found = [name for name, r in results.items() if not r['found']]
        if not_found:
            print(f"\nã€æœªç™ºè¦‹ã€‘")
            for name in not_found:
                print(f"   âŒ {name}")
                if results[name]['error']:
                    print(f"      ã‚¨ãƒ©ãƒ¼: {results[name]['error']}")

        print(f"\nğŸ“ˆ æˆåŠŸç‡: {found_count}/{total_count} ({found_count/total_count*100:.1f}%)")

        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
        print("\n" + "="*60)
        print("ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
        print("="*60)

        if found_count > 0:
            print(f"\nâœ… ç™ºè¦‹ã—ãŸRSS URLã‚’news_collector.pyã«è¿½åŠ :")
            print("```python")
            for name, result in results.items():
                if result['found'] and result['rss_urls']:
                    # æœ€åˆã®URLã‚’ä½¿ç”¨
                    rss_url = result['rss_urls'][0]
                    print(f"'{name}': {{")
                    print(f"    'rss_url': '{rss_url}',")
                    print(f"    'country': 'å›½ã‚³ãƒ¼ãƒ‰',  # è¦è¨­å®š")
                    print(f"    'country_emoji': 'å›½æ——çµµæ–‡å­—',  # è¦è¨­å®š")
                    print(f"    'country_name_ja': 'æ—¥æœ¬èªå›½å',  # è¦è¨­å®š")
                    print(f"    'description': 'ãƒ¡ãƒ¼ã‚«ãƒ¼ç‰¹å¾´'  # è¦è¨­å®š")
                    print(f"}},")
            print("```")

        if not_found:
            print(f"\nâš ï¸  æœªç™ºè¦‹ã®ãƒ¡ãƒ¼ã‚«ãƒ¼ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’æ¤œè¨:")
            for name in not_found:
                print(f"   â€¢ {name}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    discovery = RSSDiscovery()

    print("ğŸ” è‡ªå‹•è»Šãƒ¡ãƒ¼ã‚«ãƒ¼ RSS URL è‡ªå‹•æ¢ç´¢ãƒ„ãƒ¼ãƒ«")
    print("="*60)
    print()

    # å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’æ¢ç´¢
    results = discovery.discover_all()

    # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    discovery.print_summary(results)

    print("\nâœ¨ æ¢ç´¢å®Œäº†")


if __name__ == "__main__":
    main()
