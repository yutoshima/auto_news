---
title: "DPOによる選好学習でLLMをファインチューニングする方法"
source: "Qiita (Python)"
category: "it"
published: 2026-01-31T00:49:38
url: https://qiita.com/atsushi11o7/items/b6d6bb48a2888acb961c
---

# DPOによる選好学習でLLMをファインチューニングする方法

## メタデータ

- **情報源**: Qiita (Python)
- **カテゴリ**: it
- **公開日時**: 2026年01月31日 00:49
- **URL**: [https://qiita.com/atsushi11o7/items/b6d6bb48a2888acb961c](https://qiita.com/atsushi11o7/items/b6d6bb48a2888acb961c)

## 概要

はじめに
仕事でLLMをファインチューニングする機会があり、DPOを扱ったので解説します。
DPO（Direct Preference Optimization）は、LLMを人間の選好に合わせて調整するための学習手法です。2023年にStanford大学から発表され、その...

---

*この記事は自動収集システムによって保存されました。*
